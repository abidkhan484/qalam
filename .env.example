# =============================================================================
# LLM Configuration for Verse Analysis Seeding (scripts/seed-analysis.ts)
# =============================================================================

# Backend selection: 'ollama' or 'lms' (default: ollama)
LLM_BACKEND="ollama"

# Ollama (Local LLM)
OLLAMA_BASE_URL="http://localhost:11434"
OLLAMA_MODEL="qwen3:4b"

# LM Studio (OpenAI-compatible API)
LMS_BASE_URL="http://localhost:1234"
LMS_MODEL="local-model"

# =============================================================================
# LLM Configuration for Translation Assessment (API route)
# =============================================================================

# Backend selection: 'together', 'vllm', 'ollama', or 'lms'
# - together: Together.ai cloud API (default, good for production)
# - vllm: Local vLLM server (recommended for local dev with GPU)
# - ollama: Local Ollama server
# - lms: Local LM Studio server
ASSESSMENT_BACKEND="together"

# Together.ai (Cloud API - default for production)
TOGETHER_API_KEY="your-together-api-key-here"
TOGETHER_MODEL="meta-llama/Llama-3.3-70B-Instruct-Turbo"

# vLLM (Local GPU server - recommended for local development)
# Start with: vllm serve Qwen/Qwen3-4B-Instruct --dtype auto --max-model-len 4096
VLLM_BASE_URL="http://localhost:8000"
VLLM_MODEL="Qwen/Qwen3-4B-Instruct"
